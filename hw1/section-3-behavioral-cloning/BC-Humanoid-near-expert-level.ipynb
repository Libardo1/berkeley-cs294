{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "import gym\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_layer_num = 3\n",
    "hidden_layer_size = 49"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:(160080, 376), (40020, 376), (160080, 17), (40020, 17)\n"
     ]
    }
   ],
   "source": [
    "# TODO: this should be refactored and DRY from tuning-hyperparameters-and-visualization/train_humanoid.py\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "with open('./train_test_data/humanoid_train_test.pkl', 'rb') as inf:\n",
    "    X_tv, y_tv, X_test, y_test = pickle.load(inf)\n",
    "\n",
    "tf.logging.info('{0}, {1}, {2}, {3}'.format(\n",
    "    X_tv.shape, X_test.shape, y_tv.shape, y_test.shape\n",
    "))\n",
    "\n",
    "x_plh = tf.placeholder(tf.float32, shape=[None, X_tv.shape[1]])\n",
    "y_plh = tf.placeholder(tf.float32, shape=[None, y_tv.shape[1]])\n",
    "\n",
    "with tf.name_scope('fc1'):\n",
    "    Wh_var = weight_variable([x_plh.shape.dims[1].value, hidden_layer_size])\n",
    "    bh_var = bias_variable([hidden_layer_size])\n",
    "    hh = tf.nn.sigmoid(tf.matmul(x_plh, Wh_var) + bh_var)\n",
    "\n",
    "for i in range(hidden_layer_num - 1):\n",
    "    with tf.name_scope('fc{0}'.format(i + 2)):\n",
    "        Wh_var = weight_variable([hidden_layer_size, hidden_layer_size])\n",
    "        bh_var = bias_variable([hidden_layer_size])\n",
    "        hh = tf.nn.sigmoid(tf.matmul(hh, Wh_var) + bh_var)\n",
    "\n",
    "with tf.name_scope('out'):\n",
    "    W_var = weight_variable([hidden_layer_size, y_plh.shape.dims[1].value])\n",
    "    b_var = bias_variable([y_plh.shape.dims[1].value])\n",
    "    y_pred = tf.matmul(hh, W_var) + b_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./49/3-49\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "saver.restore(sess, \"./49/3-49\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,2,\n",
      "[6018.7419806805347, 6092.0598468783646, 6108.5401256820496]\n",
      "6073.11398441\n",
      "39.0310644643\n"
     ]
    }
   ],
   "source": [
    "def pred_action(obs):\n",
    "    return y_pred.eval(feed_dict={x_plh: obs.reshape(1, -1)})\n",
    "\n",
    "env = gym.make('Humanoid-v1')\n",
    "\n",
    "totalr_list = []\n",
    "max_timesteps = 600\n",
    "for _ in range(3):\n",
    "    print(_, end=',')\n",
    "    totalr = 0\n",
    "    obs = env.reset()\n",
    "    for k in range(max_timesteps):\n",
    "        action = pred_action(obs[None,:])\n",
    "        obs, r, done, _ = env.step(action)\n",
    "        totalr += r\n",
    "        env.render()\n",
    "    env.render(close=True)\n",
    "    totalr_list.append(totalr)\n",
    "print()\n",
    "print(totalr_list)\n",
    "print(np.mean(totalr_list))\n",
    "print(np.std(totalr_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expert performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collected by running `python run_expert.py experts/Humanoid-v1.pkl Humanoid-v1 --max_timesteps 600 --num_rollouts 3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expert's return at 60 timestep is around 330.\n",
    "Expert's return at 100 timestep is around 644.\n",
    "Expert's return at 600 timestep is around 6018\n",
    "\n",
    "**NOTE (Deprecated)**: the performance is only comparable at small number of timesteps. Otherwise, likely the error grow quickly and the performance deteriorate fast.\n",
    "\n",
    "To improve:\n",
    "\n",
    "1. The NN is too simplistic\n",
    "1. Not enough training data\n",
    "1. Not training for long enough (Training is too slow currently, GPU not used effectively)\n",
    "\n",
    "**Updated Note**: tuning the network size really matters, currently it's 3 hidden layers and 49 neurons per hidden layer."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Overall, I am amazed on how the imitation works! A vivid demonstration of how good a function approximator a neural network could be, given enough layers, neurons, and data (or proper training in general) !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
